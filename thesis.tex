\documentclass[12pt,a4paper,twoside,draft]{book}

% use Libertine font
\usepackage{libertine}
\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}
\usepackage[inline]{enumitem}
\usepackage{parskip} % disable indentation for new paragraphs, increased margin-bottom instead
\usepackage[ngerman,american]{babel}
\usepackage{csquotes}

\usepackage{kit_style/kitthesiscover}

\usepackage[style=alphabetic]{biblatex}
\addbibresource{bib.bib}
\setcounter{biburlnumpenalty}{100}
\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}

\usepackage{todonotes}
\usepackage{blindtext}

\usepackage{xparse}

\usepackage{xspace}

\usepackage{listings}

% for core allocation pseudo-code mostly...
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{varwidth}
\usepackage{calc} % for \widthof
%\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{amsfonts}
\usepackage{setspace}

% pandas to_latex tables look good this way
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{subcaption}

\usepackage{placeins}

\usepackage{hyperref}


% custom commands
\newcommand{\impl}[0]{$\Rightarrow$}

\widowpenalty100000
\clubpenalty100000
\raggedbottom

\begin{document}
\frontmatter
\unitlength1cm
\selectlanguage{american}

\title{Low-Latency Synchronous I/O For OpenZFS Using Persistent Memory}
\author{Christian Schwarz}
\thesistype{ma}
\primaryreviewer{Prof.\ Dr.\ Frank Bellosa}
\advisor{M.\ Sc.\ Lukas Werling}{}
\thesisbegindate{TODO}
\thesisenddate{TODO}
\maketitle

\input{kit_style/declaration}

\chapter{Abstract}
\blindtext

\mainmatter
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Contents}
\tableofcontents

\chapter{Introduction}

\chapter{Background}
\section{Synchronous I/O}
\section{Persistent Memory}
\section{OpenZFS}

\chapter{Why ZIL-LWB Is Slow On PMEM}

\chapter{ZIL-PMEM: Design \& Implementation}

\section{Project Scope}

\subsection{Goals}

\newcommand{\csgoal}[1]{\textbf{#1}}

\csgoal{Coexistence}
ZIL-PMEM must be able to coexist with ZIL-LWB due to limited availability of PMEM and limitations of the ZIL-PMEM design.

\csgoal{Same Guarantees}
ZIL-PMEM must maintain the same crash consistency guarantees towards user-space as ZIL-LWB.

\csgoal{Simple Administration \& Pooled Storage}
Pooling of storage resources and simple administration are central to ZFS's design.\cite{zfspaper}
ZFS should automatically detect that a SLOG device is PMEM and, if so, use ZIL-PMEM for all of the poolâ€™s datasets.
No further administrative action should be required to fully benefit from ZIL-PMEM.

\csgoal{Data Integrity}
Data integrity is a core feature of ZFS.\cite{zfspaper}
ZIL-PMEM must be able to detect and handle metadata and data errors as well as PMEM media failures.

\csgoal{Low Latency}
The latency overhead that ZIL-PMEM adds to the raw latency of the PMEM device should be minimal.

\csgoal{Multi-Dataset Concurrency}
Threads that run on different cores should be able to log entries to ZIL-PMEM in parallel with moderate overhead.

\csgoal{Maximum Performance on Intel Optane DC Persistent Memory}
Whereas battery-backed NVDIMMs have existed for decades, Intel Optane DC Persistent Memory is currently the only broadly available flash-based persistent memory product.
We develop ZIL-PMEM on an Intel Optane DC Persistent Memory platform and want to determine the maximum performance that can be achieved with it.

\csgoal{CPU-Efficient Handling Of PMEM Bandwidth Limits}
PMEM applies backpressure through the CPU execution pipeline if its bandwidth limits are exceeded.
Such pipeline stalls waste on-CPU time that might be used more productively by other runnable threads.
Further, \citeauthor{yang_empirical_2020} have shown that PMEM write throughput decreases at high concurrency.
Since ZIL-PMEM shares PMEM among many datasets, we expect that such overload situtations will happen in practice.
ZIL-PMEM should thus provide a mechanism to shift excessive wait time for PMEM off the CPU.

\csgoal{Testability}
ZIL-PMEM must be architected for testability.
The core algorithms presented in this chapter must be covered by unit tests.
Further, ZIL-PMEM should be integrated into the ztest user-space stress test as well as the SLOG tests of the ZFS Test Suite.

\subsection{Out Of Scope}

\csgoal{Data Redundancy}
While ZIL-PMEM must provide data integrity protections we do not require mechanisms for data redundancy.
We believe that our design can be extended to accomodate this feature.

\csgoal{Support For OpenZFS Native Encryption}
The ZIL-PMEM design presented in this section does not account for OpenZFS native encryption.
Presently, log records for encrypted datasets are stored in plain text.
However, we believe that our design can be trivially extended to accomodate this feature, albeit at possibly significant performance costs.

\subsection{Non-Goals}

\csgoal{Support For \lstinline{WR_INDIRECT}}
ZIL-LWB logs large write records' data directly to the main pool devices.
The ZIL record then only contains metadata (e.g. mtime) and a block pointer to the location in the main pool.
This technique avoids double-writes if the pool does not have a SLOG.
If the pool has a SLOG, indirect writes do not benefit from the SLOGs latency, but reduce SLOG bandwidth requirements.
The functionality is exposed to the administrator as the \lstinline{logbias} property.
We do not believe that the potential benefits of \lstinline{WR_INDIRECT} justify the additional implementation complexity.
\footnote{In fact, we identified several edge-cases with \lstinline{WR_INDIRECT} where ZIL-LWB leaks the unreplayed data blocks.\cite{zil_lwb_block_leak_edge_case}} % https://github.com/openzfs/zfs/issues/11363


\csgoal{Space Efficiency}
ZIL-PMEM should trade space for time and simplicity whenever presented with the option.
Our rationale is that PMEM capacities are significantly higher than DRAM.
For example, the smallest Intel Optane DC Persistent Memory DIMM offered by Intel is 128GiB in size.\cite{optanepricing_missing}

\section{High-Level Overview}
We introduce the concept of ZIL kinds to ZFS.
The ZIL kind is a pool-scoped variable that determines the strategy for persisting ZIL entries.
A zpool's ZIL kind is determined by the following rule:
if the pool has exactly one SLOG and that SLOG is PMEM, the ZIL kind is "ZIL-PMEM". Oterwise, it is "ZIL-LWB".
ZIL-LWB is the current ZIL's persistence implementation. It uses the SPA's metaslab allocator to allocate log-write blocks ("LWB"s) from the storage pool with a bias towards SLOG devices.
ZIL-PMEM disables metaslab allocation for its PMEM SLOG deviceand uses the resulting free PMEM space directy.

The PMEM space is partitioned into fixed-size segments.
Each segment has a corresponding in-DRAM data structure called "chunk" that tracks the segment's kernel-virtual base address and length.
Chunks are organized in a pool-wide in-DRAM data structure called PRB.
The PRB is the central code module of ZIL-PMEM.
PRB implements a high-performance crash-consistent, scalable, append-only, data-corruption-checking and garbage-collected write-ahead log that uses the PMEM chunks' segments for persistence.
After a system crash, a new instance of PRB is instantiated with the same chunks/segments that were used by the PRB instance that wrote the log before the crash.
A chunk that contains non-obsolete ("claimed") log entries is not re-used for logging until all of its entries have been obsoleted by replay.

PRB is a pool-wide resource but the ZIL is a per-head-dataset log.
Each ZIL is written replayed independently.
PRB thus maintains a per-log in-DRAM data structure called HDL in which it stores per-log state such as the log's globally unique ID, claims on chunks and replay progress.
HDLs are allocated for each of the pool's datasets on pool import.
When a dataset is created or destroyed, its HDL is allocated or freed as well.
HDL state persisted to the head dataset's ZIL header during txg sync, allowing for crash-safe replay.

PRB log entries consist of an opaque variable-sized body and a plain fixed-size header.
A log entry is always written to exactly one of the PRBs chunks' PMEM segment.
The PMEM segment is organized as a contiguous append-only sequence of entries.
Starting at offset zero, the sequence can be traversed using the length information stored in the entry header.
It is terminated by an invalid header.
If a chunk's remaining space is too small to fit an entry, the writer puts the chunk on a "full list" where it sits until all of the full chunk's entries are obsoleted by txg sync.
Once all entries are obsolete, the full chunk is moved to the "free list" for re-use by a writer.
Each entry header sotres sufficient metadata to
\begin{itemize}
\item attribute entries to a dataset, % via an ID repeated in the ZIL header and each entry's header
\item detect data corruption and missing entries,
\item order entries for replay based on sequence numbers.
\end{itemize}
The physical location of entries in PMEM is irrelevant for replay.
Instead, replay traverses each chunk's entries, filters them for the given dataset and constructs a replay sequence in DRAM using the ordering information stored in the entry headers.

PRB is integrated into the existing ZIL by \lstinline{zilog_pmem_t} which acts as an adaptor between PRB/HDL and the ZIL's "ITX layer".
The ITX layer is shared among all ZIL kinds.
It defines the format of individual log records (i.e. the content of the PRB entry's body) and manages the in-DRAM structures that track which log entries need to be persisted when sync semantics are requested by a syscall.
% for a whole dataset (\lstinline{sync()}), a single file (\lstinline{fsync()}) or an individual syscall (e.g. \lstinline{write()} on an \lstinline{O_SYNC} file descriptors).
However, the actual work of persisting entries is delegated to a ZIL-kind-specific object, i.e., \lstinline{zilog_lwb_t} for ZIL-LWB and \lstinline{zilog_pmem_t} for ZIL-PMEM.
Whereas \lstinline{zilog_lwb_t} implements all of its persistence logic in-line, \lstinline{zilog_pmem_t} is a thin wrapper around the methods of PRB/HDL.

\section{PMEM-aware SPA \& VDEV layer}
\section{ZIL kinds}
\section{PRB/HDL: A generic logical WAL for a zpool}
\section{ZIL-PMEM}
\section{ITXG Bypass For ZVOL}

\chapter{Evaluation}\label{ch:eval}
\section{Correctness}
\subsection{PRB}
\subsection{ZIL-PMEM}
\section{Performance}
\subsection{Write Performance}
\subsubsection{4k Random Sync Writes}
\subsubsection{Application Benchmarks}
\subsection{Replay Performance}

\chapter{Related Work}

\backmatter

\chapter{Appendix}\label{ch:appendix}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\emergencystretch=1em
\printbibliography

\end{document}
